---
categories:
  - "Technical Analysis"
date: 2019-04-29
description: "Current thinking on what CI/CD is and how to design an effective CI/CD pipeline"
tags:
  - "CI/CD"
  - "Deployment"
  - "Integration"
  - "Release Management"
title: "On continuous integration and delivery"
github-issue: 37
draft: true
contributors:
  - "Andrew Howden"
---

= Continuous integration & delivery

Recently I have had the opportunity to review and use a number of different approaches to continuous integration
and delivery, or CI/CD. Most recently I've encountered:

1. CI/CD implemented in BitBucket Pipelines; version controlled, stable & ignored.
2. CI/CD implemented in Jenkins scripts in the GUI, consumed by several teams and fairly fragile.
3. No CI/CD at all -- manual releases. But with the ambition to do CI/CD.

In each case each team had a set of reasons for arriving at their particular CI/CD implementation (or lack thereof).
I have set up several pipelines in the past, from using Jenkins to Travis, BitBucket and finally settling on my
preferred solution of DroneCI. Through those years there has been many lessons as to how I try and construct these
pipelines, as well as what I expect from them once they're built.

To ensure that  we're on the same page let's start by defining CI/CD:

NOTE: This will be an opinionated take on CI/CD based on my experience. As always, I'm still learning and
      https://github.com/littlemanco/www.littleman.co/issues/new/choose[feedback is welcome.]

== Definitions

One of the early adopters and more vocal organisational proponents of Continuous Integration defines CI as:

> Continuous Integration (CI) is a development practice that requires developers to integrate code into a shared 
> repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect 
> problems early^cite:[tw.ci]^

They go on to define Continuous Delivery:

> Continuous Delivery is the natural extension of Continuous Integration, an approach in which teams ensure that every 
> change to the system is releasable, and release any version with the push of a button^cite:[tw.cd]^.

With that out the way it's worth understanding how CI/CD came to be.

== A little history

Though the iterative model of software development can be traced back to the 1950's^cite:[cl.history-iteration]^ it was
previously more common for software development to go through distinct phases, not unlike a construction or other major
project:

image:/images/continuous-integration-and-delivery/waterfall-release-pattern.png[Requirements → System Design → Implementation → Integration & Testing → Deployment → Maintenance]

These phases presumed a stable set of requirements and market and had long periods of time where work from multiple
teams was merged together and tested by the QA team. There were issues with approach however; to quote Martin Fowler:

> My manager, part of the QA group, gave me a tour of a site and we entered a huge depressing warehouse stacked full 
> with cubes. I was told that this project had been in development for a couple of years and was currently integrating,
> and had been integrating for several months. My guide told me that nobody really knew how long it would take to finish
> integrating^cite:[mf.ci]^.

Beyond the integration pain, other challenges that occurred were the contrast between stable requirements and a rapidly
shifting consumer expectations for software systems and the difficulty maintaining software following it's initial
release. The later a bug is discovered in the Software Development Life Cycle (SDLC) the more expensive it is, with
some estimations being that a bug found in the "testing" phase is up to 15 times more expensive than a bug found in 
the "design" phase^cite:[syn.costs]^.

As a mechanism to address these and other issues with the SDLC Kent Beck developed the "Extreme Programming" approach,
the precursor to the "Agile" software development approach^cite:[mf.xp]^. Among other things XP endorsed the use of
continuous integration as a mechanism to ensure that integration pain was much reduced, bugs were caught early and the
software could be adjusted to new requirements. Kent used this approach with various projects including the
"Chrysler Comprehensive Compensation" project with Martin Fowler^cite:[mf.ci]^, who would later go on to publish and 
popularize this approach.

While not a requirement for continuous integration^cite:[mf.ci]^, CI servers became a popular method of implementing
this approach. ThoughtWorks were among the first with their "Cruise Control" serer, released in 2001^cite:[wiki.cc]^.
Later Sun Microsystems developed "Hudson", released in 2015^cite:[wiki.hudson]^, the precursor to the infinitely
popular and still widely used Jenkins CI. Jenkins was developed as a result of disputes with Suns purchasers 
Oracle^cite:[wiki.hudson]^ and released in 2011^cite:[wiki.jenkins]^.

Jenkins later introduced the concept of "Workflows", which were builds that could be checked in to version 
control instead of configured in a GUI^cite:[tc.ci]^ and marking the shift to CI systems entirely driven by VCS.

It is difficult to map when containers started being used as environments in which to run CI jobs as the history
of containers goes far further back than one might imagine^cite:[lmc.containers]^ however the earliest development
of the Docker plugin for Jenkins goes back to 2014^cite:[gh.j.dp]^. The rise of the popularity of Docker and its
excellent ephemeral environment tooling mean that many modern CI systems consume Docker containers as their
only build environment^cite:[gl.ee.ci],cite:[bb.ci],cite:[dr.docs]^

== CI/CD as part of the SDLC

TIP: Tangentially related are cloud build services such as https://cloud.google.com/cloud-build/[Google Cloud Build] and 
     https://aws.amazon.com/codebuild/[Amazon Build Service]. While they're interesting, they're missing several
     important features of CI.

While the historical context of how CI/CD was implemented is super useful for providing understanding of the pressures
that generated CI understanding how it fits into the SDLC first requires an understanding of how the SDLC is currently
architected. In both this and my last company we used a version of the feature branch workflow^cite:[at.g.w]^.

=== Change life cycle

Prior to any development work being completed someone would write a "user story" or "bug report". These are tickets that
describe either a new application behaviour or some circumstance in which the application does not behave as expected.
The developer reads and contributes to the ticket determining if the should be updates made to the application,
documentation or whether some tutelage is required to repair the problem in another system.

Presuming the ticket is accepted for work the developer would add the ticket to the queue and, when that ticket comes
up, begin implementation.

Implementation starts by updating the local copy of version control. The canonical branch in all version control systems
I use is called `master` and is the branch that is either deployed on production systems directly or used as a base
to create a "release branch" to be deployed. It is the version of the code on which all future work should be based:

[source,bash]
----
# Switch to the master branch
$ git checkout master

# Pull any recent changes into the local copy of master
$ git pull
----

Immediately after ensuring my local branch is up to date I switch to a new "feature" branch. This branch is
temporary and allows me to make changes to the codebase in a way that is not going to affect other people:

[source,bash]
----
# Create a feature branch
$ git checkout -b my-awesome-feature
----

From here, I do development:

[source,bash]
----
$ cat <<EOF > index.php
<?php
echo "Hello World!"
EOF
----

Once development has been completed and tests have been written I commit the changes and push the branch up to the
origin to save my work

[source,bash]
----
$ git add index.php
$ git commit -m "Add my awesome feature"
$ git push origin my-awesome-feature
----

I will then create a pull request^cite:[gh.pr]^:

image:/images/continuous-integration-and-delivery/pull-request.png[Pull Request]

It's here where we first see continuous integration checks run. In the image above we can see "Some checks have not
completed yet", with a note that Drone CI is running a "PR" job. It is here that CI is responsible for enforcing
code safety checks such as unit tests, integration tests and if the tests have been built well enough smoke tests:

image:/images/continuous-integration-and-delivery/ci-tool-running-checks.png[CI tool running checks]

Following the successful completion of these tests the PR is sent to a colleague for manual. If that colleague is
happy with the changes they will merge the work into the mainline.

Once the work is merged in to the mainline CI is responsible for deploying the work to a production system:

image:/images/continuous-integration-and-delivery/ci-tool-running-deployment.png[CI tool running deployment]

That's it! Around 3 minutes after I (or my colleague) has merged code in the work is in production and facing
users.

== Architecting a CI system

=== Responsibilities

1. Lint
2. Test
3. Smoke test
4. Deploy (using external tooling)
5. Roll back

=== Writing the jobs

== CI staying successful

(From Martin Fowler):
- Maintain a Single Source Repository.
- Automate the Build
- Make Your Build Self-Testing
- Everyone Commits To the Mainline Every Day
- Every Commit Should Build the Mainline on an Integration Machine
- Fix Broken Builds Immediately
- Keep the Build Fast
- Test in a Clone of the Production Environment
- Make it Easy for Anyone to Get the Latest Executable
- Everyone can see what's happening
-- Read: This means it's a communication tools
- Automate Deployment

(Mine)
- Integrated into existing tooling (GitHub, PRs)
- Simple & Clear
- Fast!
-- Parallel
- Reliable!!
- Handles the deployment as a result of the checks
- Can be promoted

== References  

bibliography::[]