---
categories:
  - "Technical Analysis"
date: 2019-04-29
description: "Current thinking on what CI/CD is and how to design an effective CI/CD pipeline"
tags:
  - "CI/CD"
  - "Deployment"
  - "Integration"
  - "Release Management"
title: "On continuous integration and delivery"
github-issue: 37
contributors:
  - "Andrew Howden"
---

= Continuous integration & delivery

Recently I have had the opportunity to join teams employing a number of different approaches to _continuous
Integration and delivery_ (CI/CD).

1. *Version Control*: Scripts embedded in the git repository, executed as a result of git events
2. *GUI*: Scripts written and uploaded to a server, executed on arbitrary events (time, git hooks)
3. *None*: Scripts sitting on a bastion server, executed as required

In all cases the team had a set of reasons for employing the solution the did, as well as stories as to why that
Particular solution works best for them.

Because I have both been able to get a view across these teams and have in the past build several systems that
reflect each design I felt it reasonable to try and explain how I design them now, given that experience,
as well as what the characteristics of a "good" pipeline are. 

To ensure that  we're on the same page let's start by defining CI/CD:

CAUTION: This article presumes a git repository layout as defined in 
        https://www.littleman.co/articles/laying-out-a-git-repository/[Laying out a git repository]

== Definitions

One of the early adopters and more vocal organisational proponents of Continuous Integration, Thoughtworks, 
defines CI as:

> ...a development practice that requires developers to integrate code into a shared repository several times
> a day. Each check-in is then verified by an automated build, allowing teams to detect problems 
> early^cite:[tw.ci]^

They go on to define Continuous Delivery:

> Continuous Delivery is the natural extension of Continuous Integration, an approach in which teams ensure that every 
> change to the system is releasable, and release any version with the push of a] button^cite:[tw.cd]^.

With that out the way it's worth understanding how CI/CD came to be.

== A little history

Though the iterative model of software development can be traced back to the 1950's^cite:[cl.history-iteration]^ it was
previously more common for software development to go through distinct phases, not unlike a construction or other major
project:

image:/images/continuous-integration-and-delivery/waterfall-release-pattern.png[Requirements → System Design → Implementation → Integration & Testing → Deployment → Maintenance]

These phases presumed a stable set of requirements and market and had long periods of time where work from multiple
teams was merged together and tested by the QA team. There were issues with approach however; to quote Martin Fowler:

> My manager, part of the QA group, gave me a tour of a site and we entered a huge depressing warehouse stacked full 
> with cubes. I was told that this project had been in development for a couple of years and was currently integrating,
> and had been integrating for several months. My guide told me that nobody really knew how long it would take to finish
> integrating^cite:[mf.ci]^.

Beyond the integration pain, other challenges that occurred were the contrast between stable requirements and a rapidly
shifting consumer expectations for software systems and the difficulty maintaining software following it's initial
release. The later a bug is discovered in the Software Development Life Cycle (SDLC) the more expensive it is, with
some estimations being that a bug found in the "testing" phase is up to 15 times more expensive than a bug found in 
the "design" phase^cite:[syn.costs]^.

As a mechanism to address these and other issues with the SDLC Kent Beck developed the "Extreme Programming" approach,
the precursor to the "Agile" software development approach^cite:[mf.xp]^. Among other things XP endorsed the use of
continuous integration as a mechanism to ensure that integration pain was much reduced, bugs were caught early and the
software could be adjusted to new requirements. Kent used this approach with various projects including the
"Chrysler Comprehensive Compensation" project with Martin Fowler^cite:[mf.ci]^, who would later go on to publish and 
popularize this approach.

While not a requirement for continuous integration^cite:[mf.ci]^, CI servers became a popular method of implementing
this approach. ThoughtWorks were among the first with their "Cruise Control" serer, released in 2001^cite:[wiki.cc]^.
Later Sun Microsystems developed "Hudson", released in 2015^cite:[wiki.hudson]^, the precursor to the infinitely
popular and still widely used Jenkins CI. Jenkins was developed as a result of disputes with Suns purchasers 
Oracle^cite:[wiki.hudson]^ and released in 2011^cite:[wiki.jenkins]^.

Jenkins later introduced the concept of "Workflows", which were builds that could be checked in to version 
control instead of configured in a GUI^cite:[tc.ci]^ and marking the shift to CI systems entirely driven by VCS.

It is difficult to map when containers started being used as environments in which to run CI jobs as the history
of containers goes far further back than one might imagine^cite:[lmc.containers]^ however the earliest development
of the Docker plugin for Jenkins goes back to 2014^cite:[gh.j.dp]^. The rise of the popularity of Docker and its
excellent ephemeral environment tooling mean that many modern CI systems consume Docker containers as their
only build environment^cite:[gl.ee.ci],cite:[bb.ci],cite:[dr.docs]^

== CI/CD as part of the SDLC

NOTE: Tangentially related are cloud build services such as https://cloud.google.com/cloud-build/[Google Cloud Build] and 
     https://aws.amazon.com/codebuild/[Amazon Build Service]. While they're interesting, they're missing several
     important features of CI.

While the historical context of how CI/CD was implemented is super useful for providing understanding of the pressures
that generated CI understanding how it fits into the SDLC first requires an understanding of how the SDLC is currently
architected. In both this and my last company we used a version of the feature branch workflow^cite:[at.g.w]^.

=== Change life cycle

Prior to any development work being completed someone would write a "user story" or "bug report". These are tickets that
describe either a new application behaviour or some circumstance in which the application does not behave as expected.
The developer reads and contributes to the ticket determining if the should be updates made to the application,
documentation or whether some tutelage is required to repair the problem in another system.

Presuming the ticket is accepted for work the developer would add the ticket to the queue and, when that ticket comes
up, begin implementation.

Implementation starts by updating the local copy of version control. The canonical branch in all version control systems
I use is called `master` and is the branch that is either deployed on production systems directly or used as a base
to create a "release branch" to be deployed. It is the version of the code on which all future work should be based:

[source,bash]
----
# Switch to the master branch
$ git checkout master

# Pull any recent changes into the local copy of master
$ git pull
----

Immediately after ensuring my local branch is up to date I switch to a new "feature" branch. This branch is
temporary and allows me to make changes to the codebase in a way that is not going to affect other people:

[source,bash]
----
# Create a feature branch
$ git checkout -b my-awesome-feature
----

From here, I do development:

[source,bash]
----
$ cat <<EOF > index.php
<?php
echo "Hello World!"
EOF
----

Once development has been completed and tests have been written I commit the changes and push the branch up to the
origin to save my work

[source,bash]
----
$ git add index.php
$ git commit -m "Add my awesome feature"
$ git push origin my-awesome-feature
----

I will then create a pull request^cite:[gh.pr]^:

image:/images/continuous-integration-and-delivery/pull-request.png[Pull Request]

It's here where we first see continuous integration checks run. In the image above we can see "Some checks have not
completed yet", with a note that Drone CI is running a "PR" job. It is here that CI is responsible for enforcing
code safety checks such as unit tests, integration tests and if the tests have been built well enough smoke tests:

image:/images/continuous-integration-and-delivery/ci-tool-running-checks.png[CI tool running checks]

Following the successful completion of these tests the PR is sent to a colleague for manual. If that colleague is
happy with the changes they will merge the work into the mainline.

Once the work is merged in to the mainline CI is responsible for deploying the work to a production system:

image:/images/continuous-integration-and-delivery/ci-tool-running-deployment.png[CI tool running deployment]

That's it! Around 3 minutes after I (or my colleague) has merged code in the work is in production and facing
users.

== The building blocks of CI/CD

Given the above development workflow we can see our CI/CD system has several different responsibilities, split into
two areas of focus:

1. Ensuring the program remains correct before work is merged in to the mainline
2. Making the new software available to users

This work is usually broken up into a staged process. The following is an example of the "Pull request" process:

image:/images/continuous-integration-and-delivery/pipeline-simple.png[A set of build steps executed sequentially]

These sets of processes are usually referred to as a "Pipeline", named after the computing definition:

> a set of data processing elements connected in series, where the output of one element is the input of the next 
  one^cite:[wiki.pipeline]^.

There are several different components that make up the CI/CD pipeline^cite:[wiki.linter],cite:[wiki.ut],cite:[wiki.it]^:

[cols="2,8"]
|===
| Tool                 | Purpose

| Version Control      | Contain the current state of the software, be that the canonical version or a patch that has
                         been proposed _e.g. `git`, `svn`._
| Linter               | A tool that analyzes source code to flag programming errors, bugs, stylistic errors, and 
                         suspicious constructs _e.g. `phpcs`, `yamllint`._ 
| Unit Tests           | A software testing method by which individual units of source code, sets of one or more 
                         computer program modules together with associated control data, usage procedures, and operating
                         procedures, are tested to determine whether they are fit for use
                         _e.g. `phpunit`._  
| Integration Tests    | The phase in software testing in which individual software modules are combined and tested as 
                         a group.
| Task Runner          | A tool that allows aggregating tasks such as application compilation, testing and deployment
                         behind a single interface _e.g. `make`, `robo`._
| Deployment Manager   | A tool that manages replicas of the software deployed for production, facing or testing
                         purposes behind a single interface _e.g. `helm`._
|===

These tools should all be set up to be consumed both in and outside the CI/CD pipeline in exactly the same way. The
pipeline itself should be as "dumb" as possible, only invoking the task runner in an environment with the tools
available to complete the desired action:

image:/images/continuous-integration-and-delivery/pipeline-design.png[CI tool consumes task runner which does logic]

Practically speaking this means instead of configuring pipelines with large sets of commands:

[source,bash]
----
echo $B64_GOOGLE_SERVICE_ACCOUNT | base64 -d > $GOOGLE_APPLICATION_CREDENTIALS
gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
gcloud config set project $GOOGLE_PROJECT_NAME
gcloud container clusters get-credentials --zone $GOOGLE_PROJECT_REGION $GOOGLE_GKE_CLUSTER_NAME
helm upgrade --install --namespace www-littleman-co www-littleman-co deploy/helm
----

The pipeline should be invoked only with a single command with some argument indicating which task to run:

[source,bash]
----
robo deploy --environment=production
----

This separation of responsibilities into the task runner, deployment manager and CI pipeline allows the reuse and 
debugging of the majority of the pipeline locally, allows switching between CI/CD services without undue cost and makes
the pipelines as simple and predictable as possible.

TIP: Practically there can be some setup required to authorize services and do other environment specific setup in
     CI/CD pipelines. This is nonsensical in a task runner but also should not be required on each job. In principle
     the environment itself should consume credentials and set up access without work required for the particular job.
     At the time of writing it looks like this is only possible with the use of
     http://plugins.drone.io/[DroneCIs plugin system].

== Writing the task
  
As discussed, the pipeline consists of a set of steps that need to be executed prior to verifying the codebase is still
correct or deploying the software to some environment. Further, these steps should be executable both on the local
machine and in the build pipeline in exactly the same way. The way to implement this is via a task runner, such as
https://robo.li[Robo]:

[source,php]
----
$ robo init
  ~~~ Welcome to Robo! ~~~~ 
  RoboFile.php will be created in the current directory 
  Edit this file to add your commands! 

$ cat RoboFile.php 
<?php
class RoboFile extends \Robo\Tasks {}
----

We then add our tasks in our task runners domain specific language; in this case, by writing a public function in
PHP:

[source,php]
----
class RoboFile extends \Robo\Tasks
{
    /**
     * Runs lints over the codebase
     * 
     * @option files A space separated list of files to lint
     */
    public function lint()
    {   
        $this->taskExecStack()
            ->stopOnFail()
            ->exec('yamllint .')
            ->run();
    }
}
----

That allows us to test the task locally:

[source,bash]
----
$ robo lint
 [ExecStack] yamllint www.littleman.co
 [ExecStack] Running yamllint www.littleman.co
www.littleman.co/config.yaml
  24:3      error    wrong indentation: expected 4 but found 2  (indentation)
  42:81     error    line too long (101 > 80 characters)  (line-length)
  46:81     error    line too long (123 > 80 characters)  (line-length)
  ...
----

And adjust the lint (or the code) until it works as expected:

[source,bash]
----
cat <<EOF > .yamllint
---
extends: default

ignore: |
  deploy/helm

rules:
  line-length:
    max: 120
  braces:
    max-spaces-inside: 1
EOF
----

The task should work successfully before being committed:

[source,bash]
----
$ robo lint
 [ExecStack] yamllint .
 [ExecStack] Running yamllint .
 [ExecStack] Done in 0.171s

$ echo $?
0           # Success
----

Once the step is successful and committed it is ready to be consumed in the build pipeline. Steps can be written
for any number of tasks:

[source,bash]
----
$ robo list
....
Available commands:
  deploy          
  help            Displays help for a command
  integration     
  lint            Runs lints over the codebase
  list            Lists commands
  test            
----

However, they should all work and be useful locally before they're consumed in the pipeline.

== Designing the pipeline

=== Choosing a CI/CD server

There are many different CI/CD tools that are available, both implemented as open source and commercially supported.
However, some tools are better than others. To me, the most important characteristics are:

1. Driven by version control
2. Simple & Clear
3. Docker based
4. Well integrated into project management tooling
5. Capable of building a "Directed acyclic graph" (DAG)

There are several tools that match this criteria, including:

1. Drone CI
2. Circle CI
3. BitBucket Pipelines
4. GitLab Pipelines

There are likely many more. However, for the purpose of this tutorial we'll be adopting Drone CI.

=== Writing the job

Most tools specify build configuration with a file or folder in the project root, and Drone is no exception with its
`drone.yml` specification.

A minimal configuration that lints every time the code is pushed to the upstream server would look like:

[source,bash]
----
$ cat <<EOF > .drone.yml
---
kind: "pipeline"
name: "lint"
steps:
  - name: "lint"
    image: "debian:stretch"
    commands:
      - robo lint
EOF
----

Committing and pushing it should trigger the build:

[source,bash]
----
# git add RoboFile.php .drone.yml
$ git commit -m "Create an initial build configuration"
$ git push origin ad-hoc-ci-cd-demo
----

Unfortunately this by itself does not work:

image:/images/continuous-integration-and-delivery/failing-build-robo-not-found.png[A build failure due to the missing "robo" binary]

While the job is executed, the build environment is the tools to invoke the task runner and do the required `${WORK}`.
There are two possible solutions to this:

1. Install the tools in the pipeline
2. Create a container that includes the build tools

Of these options. #2 is far better. It allows much faster builds, prevents flaky builds due to software upstreams being
down or the network being flaky and fixes the build software to a "known" version.

Often, there are pre made images that are suitable for our purpose. However, unless the heritage of that image is
trusted its better to simply write one. If the build task is simple and it's possible to write an image that does
a single thing it's reasonable to publish the image. However, more likely the build will require some custom tools
not regularly bundled together.

An example would be:

[source,dockerfile]
----
FROM debian:stretch

# Some system upgrades
RUN apt-get update && \
    apt-get dist-upgrade --yes

RUN apt-get install --yes \
    # "Basic" tools 
    curl \
    python python-pip \
    # Task runner runtime
    php && \
    # Linting Tools
    pip install \
        yamllint

# Install the task runer
RUN curl -O https://robo.li/robo.phar && \
    chmod +x ./robo.phar && \
    mv ./robo.phar /usr/local/bin/robo
----

The logic to build (and push) this new Dockerfile can be embedded in the same task runner:

[source,php]
----

    /**
     * Builds containers. Available containers are those at the path "build/containers"
     *
     * @option container The container to build
     */
    public function containerBuild($opts = ['container' => 'web'])
    {
        $refspec       = exec('git rev-parse --short HEAD');
        $containerName = self::CONTAINER_NAMESPACE . '--' . $opts['container'];

        $this->taskDockerBuild(self::CONTAINER_PATH . DIRECTORY_SEPARATOR . $opts['container'])
            ->tag($containerName . ':' . $refspec)
            ->tag($containerName . ':latest' )
            ->run();
    }

    public function containerPush($opts = ['container' => 'web'])
    {
      // Omitted for brevity
    }
----

Once the image has been built & pushed to the repository we can consume it in our build configuration:

[source,yaml]
----
---
kind: "pipeline"
name: "lint"
steps:
  - name: "lint"
    image: "gcr.io/littleman-co/www-littleman-co--build:fe2e8b1"
    commands:
      - /usr/local/bin/robo lint
----

And the build should work as expected:

image:/images/continuous-integration-and-delivery/successful-build-robo-found.png[A successful build after robo is installed]

This repeats for however many tasks the build should consist of; lints, tests and eventually even the deployment.

TIP: In this case a private image is used but authentication is delegated to the build farm directly, rather than
     specified in the build configuration. Practically, Drone runs on top of GKE in which the docker daemon is
     pre-authorized to the container registry.

TIP: Like other docker images, it's good to keep the build containers small and suited for the task. The build container
     need not be the deploy container, nor the analysis container and so on. Such reducing of responsibility helps
     prevent unintended dependencies on the state or applications of other containers.

=== The pipeline

So far we are able to write tasks that do some analysis work on the build, already a huge step forward in our ability
to ensure system correctness over time.

However, a pipeline is not a single job; rather, it is a series of jobs that are triggered in a particular circumstance.
There are two that I use regularly:

- *Pull Request*: A workflow designed to ensure changes do not break the system. Triggered by the creation of the pull
                  request and blocking the pull request from being merged until the build is successful.
- *Deployment*:   A workflow designed to push the "known good" code to a production system

They usually look something like:

image:/images/continuous-integration-and-delivery/pipeline-dag.png[The PR and deployment pipeline graph]

The specific steps involved depend on your deployment model, available tests or other requirements. However there can
be multiple workflows and each workflow can consist of an arbitrarily complex set of steps. The pipelines are usually
invoked by some sort of "trigger" mechanism; some signal to the CI tool to start a specific pipeline. In the case of
Drone the configuration https://docs.drone.io/user-guide/pipeline/triggers/[is called just that -- triggers]. 

TIP: There are situations in which it is desirable to deploy to production in a hurry, without verifying the software
     for correctness, such as a security issue or other disaster management process. Accordingly, the production
     pipeline should be less than 5 minutes to complete and extremely reliable.

==== Pull Request

To create a "pull request" workflow, the configuration looks like:

[source,yaml]
----
---
kind: "pipeline"
name: "lint"
steps:
  - name: "lint"
    image: "gcr.io/littleman-co/www-littleman-co--build:da8b695"
    commands: 
      - /usr/local/bin/robo lint

trigger:
  # Execute this process every time a new pull request is opened
  event:
    - pull_request
----

Jobs can be executed in parallel by specifying multiple pipelines that have the same dependencies:

[source,yaml]
----
---
# Steps are omitted for brevity in the post
kind: "pipeline"
name: "lint"
steps: []
trigger:
  event:
    - pull_request
---
kind: "pipeline"
name: "unit-test"
steps: []
trigger:
  event:
    - pull_request
----

And it's possible to "gate" steps on the success on other steps by using the "depends_on" node:

[source,yaml]
----
---
# Steps are omitted for brevity in the post
kind: "pipeline"
name: "lint"
steps: []
trigger:
  event:
    - pull_request
---
kind: "pipeline"
name: "unit-test"
steps: []
trigger:
  event:
    - pull_request
---
kind: "pipeline"
name: "integration-test"
steps: []
trigger:
  event:
    - pull_request
depends_on:
  - lint
  - unit-test
---
kind: "pipeline"
name: "stress-test"
steps: []
trigger:
  event:
    - pull_request
depends_on:
  - lint
  - unit-test
---
kind: "pipeline"
name: "smoke-test"
steps: []
trigger:
  event:
    - pull_request
depends_on:
  - lint
  - unit-test
----

This gives us our ability to create our graph. So far we have generated only the first two steps:

image:/images/continuous-integration-and-delivery/dag-first-two-steps.png[The pipeline pull request initial steps]

However, we can see our pipeline working as expected:

https://youtu.be/EC5W4L7YqsI

video::EC5W4L7YqsI[youtube]

==== Deployment

Implementing the deployment task is exactly the same process, varying only in the triggers. However, one common use
case for production is being able to manually gate builds based on some human intervention. In the case of Drone
this is termed "Promotion". Given configuration that looks like:

[source,yaml]
----
---
kind: "pipeline"
name: "container"
steps:
  - name: "container"
    image: "gcr.io/littleman-co/www-littleman-co--build:d7c8edd"
    environment:
      GOOGLE_SERVICE_ACCOUNT:
        from_secret: GOOGLE_SERVICE_ACCOUNT
    # Required to build container
    privileged: true
    commands:
      # Enable img to push to docker registry
      - img login -u _json_key -p "$GOOGLE_SERVICE_ACCOUNT" https://gcr.io
      - /usr/local/bin/robo application:compile
      - /usr/local/bin/robo container:build --container=web
      - /usr/local/bin/robo container:push --container=web

trigger:
  branch:
    - master
  event:
    - push
---
kind: "pipeline"
name: "canary"
steps:
  - name: "canary"
    image: "gcr.io/littleman-co/www-littleman-co--build:d7c8edd"
    commands:
      - /usr/local/bin/robo deploy --environment=canary
trigger:
  branch:
    - master
  event:
    - push
depends_on:
  - container
---
kind: "pipeline"
name: "production"
steps:
  - name: "deploy"
    image: "gcr.io/littleman-co/www-littleman-co--build:d7c8edd"
    commands:
      - /usr/local/bin/robo deploy --environment=production
trigger:
  event:
    - promote
  target:
    - production
  branch:
    - master
---
kind: "pipeline"
name: "rollback"
steps:
  - name: "rollback"
    image: "gcr.io/littleman-co/www-littleman-co--build:d7c8edd"
    commands:
      - /usr/local/bin/robo rollback --environment=production
trigger:
  event:
    - rollback
  target:
    - production
  branch:
    - master
----

We achieve a "two step" deployment process:

image:/images/continuous-integration-and-delivery/build-flow-with-promotion.png[A two step build, with promotion]

Where step 1 is executed automatically on anything being added to the master branch, whereas step two requires a human
to "promote" the build. At the time of writing 
https://github.com/drone/drone-ui/issues/171[it is only possible to trigger the promotion via the CLI]:

[source,yaml]
----
drone build promote littlemanco/www.littleman.co 100 production
----

Where:

- `littlemanco/www.littleman.co` is the repository
- `100` is the build number
- `production` is the intended environment

TIP: Our configuration is getting very long and repetitive at this point. Drone has some support for jsonnet as a
     configuration language to help remedy this, or jsonnet can be expressed to yaml prior to the build execution
     and committed.

CAUTION: In the build configuration above there is a "rollback" step. At the time of writing this
         https://github.com/drone/drone/issues/2695[appears to be broken].

== In Conclusion

Continuous integration & delivery has become a fairly essential part of software development. It augments the software
delivery life cycle with additional correctness checking to find bugs earlier and dramatically reduces the cost and
risk associated with pushing changes to production. There are many CI/CD servers however Drone implements all required
features for the vast majority of software projects and is open source and trivially installable. With Drone we 
constructed a multi-step build cycle stubbing out lints, tests and other correctness checks common software, as well
as stubbed out a workflow for continuous delivery.

That's it! This post got a bit long, and I did not get to cover all of the things I would like to. However if you made
it this far and you enjoyed it be sure to 
https://github.com/littlemanco/www.littleman.co/issues[voice yourself on GitHub], and I'll keep writing this stuff up.

<3

== References  

bibliography::[]
